{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n",
      "(120, 3)\n",
      "(30, 3)\n"
     ]
    }
   ],
   "source": [
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "Y_train_encoded = OneHotEncoder().fit_transform(Y_train.reshape(-1, 1)).toarray()\n",
    "Y_test_encoded = OneHotEncoder().fit_transform(Y_test.reshape(-1, 1)).toarray()\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "X_test = tf.convert_to_tensor(X_test)\n",
    "Y_train_encoded = tf.convert_to_tensor(Y_train_encoded)\n",
    "Y_test_encoded = tf.convert_to_tensor(Y_test_encoded)\n",
    "\n",
    "n0, m_total = X.shape\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train_encoded.shape)\n",
    "print(Y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My naive network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y, AL, eps=1.e-7):\n",
    "    M = AL.shape[0]\n",
    "    L = tf.reduce_sum(-Y * tf.math.log(tf.clip_by_value(AL, eps, 1 - eps))) / M\n",
    "    return L\n",
    "\n",
    "def get_dAL(Y, AL, eps=1.e-7):\n",
    "    M = AL.shape[0]\n",
    "    return - Y / tf.clip_by_value(AL, eps, 1 - eps) / M\n",
    "\n",
    "def get_dA(dZ_post, W_post):\n",
    "    return dZ_post @ tf.transpose(W_post)\n",
    "\n",
    "def get_dZL(Y, AL):\n",
    "    M = AL.shape[0]\n",
    "    return (AL * tf.reduce_sum(Y, axis=1, keepdims=True) - Y) / M\n",
    "\n",
    "def get_dZ(dA, Z, g_der):\n",
    "    return dA * g_der(Z)\n",
    "\n",
    "def get_dweights(A_prev, dZ):\n",
    "    dW = tf.transpose(A_prev) @ dZ\n",
    "    db = tf.reduce_sum(dZ, axis=0)\n",
    "    return dW, db\n",
    "\n",
    "def relu_der(Z):\n",
    "    return tf.cast(Z > 0, dtype=tf.float64)\n",
    "\n",
    "def softmax_der(Z):\n",
    "    softmax_Z = softmax(Z)\n",
    "    return softmax_Z * (1 - softmax_Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    denominator = tf.reduce_sum(tf.exp(Z), axis=1, keepdims=True)\n",
    "    return tf.exp(Z) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_uniform(shape):\n",
    "    tf.random.set_seed(0)\n",
    "    weights = {}\n",
    "    for i in range(1, len(shape)):\n",
    "        weights[f'W{i}'] = tf.random.uniform((shape[i - 1], shape[i]), dtype=tf.float64)\n",
    "        weights[f'b{i}'] = tf.random.uniform((shape[i], ), dtype=tf.float64)\n",
    "    return weights\n",
    "\n",
    "def update(weights, dweights, learning_rate):\n",
    "    for key in weights.keys():\n",
    "        weights[key] -= learning_rate * dweights[f'd{key}']\n",
    "\n",
    "def forward_prop(X_train, weights):\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "\n",
    "    Zs = {}\n",
    "    As = {}\n",
    "    Z1 = X_train @ W1 + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = tf.nn.softmax(Z2)\n",
    "    Zs[\"Z1\"] = Z1\n",
    "    As[\"A1\"] = A1\n",
    "    Zs[\"Z2\"] = Z2\n",
    "    As[\"A2\"] = A2\n",
    "    return Zs, As\n",
    "\n",
    "def backward_prop(X_train, Y_train, weights, Zs, As):\n",
    "    dweights = {}\n",
    "\n",
    "    dA2 = get_dAL(Y_train, As[\"A2\"])\n",
    "    dZ2 = get_dZL(Y_train, As[\"A2\"])\n",
    "    dW2, db2 = get_dweights(As[\"A1\"], dZ2)\n",
    "\n",
    "    dA1 = get_dA(dZ2, weights[\"W2\"])\n",
    "    dZ1 = get_dZ(dA1, Zs[\"Z1\"], relu_der)\n",
    "    dW1, db1 = get_dweights(X_train, dZ1)\n",
    "    dweights[\"dW1\"] = dW1\n",
    "    dweights[\"db1\"] = db1\n",
    "    dweights[\"dW2\"] = dW2\n",
    "    dweights[\"db2\"] = db2\n",
    "    return dweights\n",
    "\n",
    "def train(X_train, Y_train, epochs=100, learning_rate=0.001, n1=10, verbose=True):\n",
    "    n0 = X_train.shape[1]\n",
    "    n2 = Y_train.shape[1]\n",
    "    weights = init_uniform((n0, n1, n2))\n",
    "    Zs, As = forward_prop(X_train, weights)\n",
    "    history = {'loss': [], 'accuracy': []}\n",
    "    for i in range(epochs):\n",
    "        dweights = backward_prop(X_train, Y_train, weights, Zs, As)\n",
    "        update(weights, dweights, learning_rate)\n",
    "        Zs, As = forward_prop(X_train, weights)\n",
    "\n",
    "        l = float(loss(Y_train, As[\"A2\"]))\n",
    "        acc = accuracy_score(tf.argmax(Y_train, axis=1), tf.argmax(As['A2'], axis=1))\n",
    "        history['loss'].append(l)\n",
    "        history['accuracy'].append(acc)\n",
    "        if verbose and i % (epochs // 10) == 0:\n",
    "            print('epoch', i + 1, 'loss:',l, 'acc:', acc)\n",
    "\n",
    "    return history, weights, Zs, As\n",
    "\n",
    "def predict(X, weights):\n",
    "    A2 = forward_prop(X, weights)[1][\"A2\"]\n",
    "    return tf.argmax(A2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 loss: 9.648960995973235 acc: 0.30833333333333335\n",
      "epoch 16 loss: 1.0212939253229603 acc: 0.36666666666666664\n",
      "epoch 31 loss: 0.9950905058417073 acc: 0.36666666666666664\n",
      "epoch 46 loss: 0.9700010244038909 acc: 0.39166666666666666\n",
      "epoch 61 loss: 0.9459320632294451 acc: 0.5583333333333333\n",
      "epoch 76 loss: 0.922826578184769 acc: 0.6666666666666666\n",
      "epoch 91 loss: 0.9006400284682189 acc: 0.6833333333333333\n",
      "epoch 106 loss: 0.8793373713275563 acc: 0.6833333333333333\n",
      "epoch 121 loss: 0.8588904263487217 acc: 0.6916666666666667\n",
      "epoch 136 loss: 0.8392756237621076 acc: 0.6916666666666667\n"
     ]
    }
   ],
   "source": [
    "history, weights, Zs, As = train(X_train, Y_train_encoded, epochs=150, learning_rate=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_gradient():\n",
    "    W1 = weights['W1']\n",
    "    b1 = weights['b1']\n",
    "    W2 = weights['W2']\n",
    "    b2 = weights['b2']\n",
    "    Z1 = X_train @ W1 + b1\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = tf.nn.softmax(Z2)\n",
    "    L = loss(Y_train_encoded, A2)\n",
    "    return tf.gradients(L, [W1, b1, W2, b2, Z1, A1, Z2, A2])\n",
    "\n",
    "dW1, db1, dW2, db2, dZ1, dA1, dZ2, dA2 = get_gradient()\n",
    "\n",
    "tol = 1.e-10\n",
    "tf.debugging.assert_near(dA2, get_dAL(Y_train_encoded, As['A2']), rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(dZ2, get_dZL(Y_train_encoded, As['A2']), rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(dW2, get_dweights(As['A1'], dZ2)[0], rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(db2, get_dweights(As['A1'], dZ2)[1], rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(dA1, get_dA(dZ2, weights['W2']), rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(dZ1, get_dZ(dA1, Zs['Z1'], relu_der), rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(dW1, get_dweights(X_train, dZ1)[0], rtol=tol, atol=tol)\n",
    "tf.debugging.assert_near(db1, get_dweights(X_train, dZ1)[1], rtol=tol, atol=tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120,), dtype=int64, numpy=\n",
       "array([2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 2, 0,\n",
       "       2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 0], dtype=int64)>"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_train, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 10)                50        \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(2)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(Y_train_encoded.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=3.253091535668539>"
      ]
     },
     "execution_count": 666,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(Y_train_encoded, tf.cast(model(X_train), dtype=tf.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "4/4 [==============================] - 0s 39ms/step - loss: 3.0993 - val_loss: 2.8421\n",
      "Epoch 2/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 2.7226 - val_loss: 2.6166\n",
      "Epoch 3/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.4429 - val_loss: 2.4884\n",
      "Epoch 4/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2682 - val_loss: 2.4142\n",
      "Epoch 5/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.1711 - val_loss: 2.3348\n",
      "Epoch 6/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.0633 - val_loss: 2.2164\n",
      "Epoch 7/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.9570 - val_loss: 2.0641\n",
      "Epoch 8/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.8215 - val_loss: 1.8964\n",
      "Epoch 9/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.7001 - val_loss: 1.7321\n",
      "Epoch 10/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.5893 - val_loss: 1.5781\n",
      "Epoch 11/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.4812 - val_loss: 1.4359\n",
      "Epoch 12/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3689 - val_loss: 1.3134\n",
      "Epoch 13/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2669 - val_loss: 1.1964\n",
      "Epoch 14/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1674 - val_loss: 1.0930\n",
      "Epoch 15/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.0729 - val_loss: 1.0038\n",
      "Epoch 16/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9858 - val_loss: 0.9262\n",
      "Epoch 17/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9122 - val_loss: 0.8533\n",
      "Epoch 18/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8516 - val_loss: 0.7945\n",
      "Epoch 19/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8014 - val_loss: 0.7516\n",
      "Epoch 20/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7672 - val_loss: 0.7188\n",
      "Epoch 21/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7398 - val_loss: 0.6981\n",
      "Epoch 22/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7199 - val_loss: 0.6852\n",
      "Epoch 23/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.7082 - val_loss: 0.6790\n",
      "Epoch 24/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6983 - val_loss: 0.6687\n",
      "Epoch 25/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6909 - val_loss: 0.6594\n",
      "Epoch 26/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6828 - val_loss: 0.6585\n",
      "Epoch 27/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6750 - val_loss: 0.6500\n",
      "Epoch 28/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6666 - val_loss: 0.6427\n",
      "Epoch 29/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6608 - val_loss: 0.6415\n",
      "Epoch 30/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6521 - val_loss: 0.6300\n",
      "Epoch 31/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6447 - val_loss: 0.6229\n",
      "Epoch 32/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6396 - val_loss: 0.6100\n",
      "Epoch 33/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6315 - val_loss: 0.6046\n",
      "Epoch 34/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6261 - val_loss: 0.6066\n",
      "Epoch 35/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6190 - val_loss: 0.6002\n",
      "Epoch 36/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6129 - val_loss: 0.5962\n",
      "Epoch 37/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6076 - val_loss: 0.5868\n",
      "Epoch 38/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.6016 - val_loss: 0.5831\n",
      "Epoch 39/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5960 - val_loss: 0.5748\n",
      "Epoch 40/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5899 - val_loss: 0.5712\n",
      "Epoch 41/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5844 - val_loss: 0.5657\n",
      "Epoch 42/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5793 - val_loss: 0.5625\n",
      "Epoch 43/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5738 - val_loss: 0.5579\n",
      "Epoch 44/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5688 - val_loss: 0.5503\n",
      "Epoch 45/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5645 - val_loss: 0.5422\n",
      "Epoch 46/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5591 - val_loss: 0.5420\n",
      "Epoch 47/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5536 - val_loss: 0.5385\n",
      "Epoch 48/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5491 - val_loss: 0.5374\n",
      "Epoch 49/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5444 - val_loss: 0.5328\n",
      "Epoch 50/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5397 - val_loss: 0.5243\n",
      "Epoch 51/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5352 - val_loss: 0.5168\n",
      "Epoch 52/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5311 - val_loss: 0.5117\n",
      "Epoch 53/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5263 - val_loss: 0.5111\n",
      "Epoch 54/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5217 - val_loss: 0.5055\n",
      "Epoch 55/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5187 - val_loss: 0.5064\n",
      "Epoch 56/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5134 - val_loss: 0.4972\n",
      "Epoch 57/150\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5093 - val_loss: 0.4929\n",
      "Epoch 58/150\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5051 - val_loss: 0.4888\n",
      "Epoch 59/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5020 - val_loss: 0.4896\n",
      "Epoch 60/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4969 - val_loss: 0.4848\n",
      "Epoch 61/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4935 - val_loss: 0.4776\n",
      "Epoch 62/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4898 - val_loss: 0.4723\n",
      "Epoch 63/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4854 - val_loss: 0.4718\n",
      "Epoch 64/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4816 - val_loss: 0.4713\n",
      "Epoch 65/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4783 - val_loss: 0.4721\n",
      "Epoch 66/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4755 - val_loss: 0.4706\n",
      "Epoch 67/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4713 - val_loss: 0.4655\n",
      "Epoch 68/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4676 - val_loss: 0.4558\n",
      "Epoch 69/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4638 - val_loss: 0.4507\n",
      "Epoch 70/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4619 - val_loss: 0.4513\n",
      "Epoch 71/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4569 - val_loss: 0.4449\n",
      "Epoch 72/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4542 - val_loss: 0.4376\n",
      "Epoch 73/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4509 - val_loss: 0.4361\n",
      "Epoch 74/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4472 - val_loss: 0.4360\n",
      "Epoch 75/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4443 - val_loss: 0.4339\n",
      "Epoch 76/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4411 - val_loss: 0.4368\n",
      "Epoch 77/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4379 - val_loss: 0.4348\n",
      "Epoch 78/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4346 - val_loss: 0.4271\n",
      "Epoch 79/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4312 - val_loss: 0.4217\n",
      "Epoch 80/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4291 - val_loss: 0.4158\n",
      "Epoch 81/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4264 - val_loss: 0.4131\n",
      "Epoch 82/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4226 - val_loss: 0.4186\n",
      "Epoch 83/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4200 - val_loss: 0.4208\n",
      "Epoch 84/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4170 - val_loss: 0.4129\n",
      "Epoch 85/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4134 - val_loss: 0.4077\n",
      "Epoch 86/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4108 - val_loss: 0.4066\n",
      "Epoch 87/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4074 - val_loss: 0.3988\n",
      "Epoch 88/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4046 - val_loss: 0.3946\n",
      "Epoch 89/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4019 - val_loss: 0.3918\n",
      "Epoch 90/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3992 - val_loss: 0.3900\n",
      "Epoch 91/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3963 - val_loss: 0.3891\n",
      "Epoch 92/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3940 - val_loss: 0.3894\n",
      "Epoch 93/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3905 - val_loss: 0.3887\n",
      "Epoch 94/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3893 - val_loss: 0.3907\n",
      "Epoch 95/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3852 - val_loss: 0.3833\n",
      "Epoch 96/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3835 - val_loss: 0.3736\n",
      "Epoch 97/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3798 - val_loss: 0.3725\n",
      "Epoch 98/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3771 - val_loss: 0.3714\n",
      "Epoch 99/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3752 - val_loss: 0.3685\n",
      "Epoch 100/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3723 - val_loss: 0.3720\n",
      "Epoch 101/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3708 - val_loss: 0.3733\n",
      "Epoch 102/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3669 - val_loss: 0.3676\n",
      "Epoch 103/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3637 - val_loss: 0.3592\n",
      "Epoch 104/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3611 - val_loss: 0.3513\n",
      "Epoch 105/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3610 - val_loss: 0.3455\n",
      "Epoch 106/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3577 - val_loss: 0.3453\n",
      "Epoch 107/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3536 - val_loss: 0.3507\n",
      "Epoch 108/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3509 - val_loss: 0.3558\n",
      "Epoch 109/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3509 - val_loss: 0.3600\n",
      "Epoch 110/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3473 - val_loss: 0.3506\n",
      "Epoch 111/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3449 - val_loss: 0.3398\n",
      "Epoch 112/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3433 - val_loss: 0.3322\n",
      "Epoch 113/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3395 - val_loss: 0.3331\n",
      "Epoch 114/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3367 - val_loss: 0.3338\n",
      "Epoch 115/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3343 - val_loss: 0.3349\n",
      "Epoch 116/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3319 - val_loss: 0.3337\n",
      "Epoch 117/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3295 - val_loss: 0.3302\n",
      "Epoch 118/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3278 - val_loss: 0.3216\n",
      "Epoch 119/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3249 - val_loss: 0.3185\n",
      "Epoch 120/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3228 - val_loss: 0.3200\n",
      "Epoch 121/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3210 - val_loss: 0.3216\n",
      "Epoch 122/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3177 - val_loss: 0.3171\n",
      "Epoch 123/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3158 - val_loss: 0.3118\n",
      "Epoch 124/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3144 - val_loss: 0.3079\n",
      "Epoch 125/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3113 - val_loss: 0.3133\n",
      "Epoch 126/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3093 - val_loss: 0.3132\n",
      "Epoch 127/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.3067 - val_loss: 0.3110\n",
      "Epoch 128/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3043 - val_loss: 0.3074\n",
      "Epoch 129/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3021 - val_loss: 0.3019\n",
      "Epoch 130/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.3003 - val_loss: 0.2971\n",
      "Epoch 131/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2978 - val_loss: 0.2961\n",
      "Epoch 132/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2955 - val_loss: 0.2960\n",
      "Epoch 133/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2935 - val_loss: 0.2981\n",
      "Epoch 134/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2916 - val_loss: 0.2966\n",
      "Epoch 135/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2894 - val_loss: 0.2928\n",
      "Epoch 136/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2873 - val_loss: 0.2902\n",
      "Epoch 137/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2850 - val_loss: 0.2838\n",
      "Epoch 138/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2838 - val_loss: 0.2791\n",
      "Epoch 139/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2813 - val_loss: 0.2789\n",
      "Epoch 140/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2808 - val_loss: 0.2855\n",
      "Epoch 141/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2772 - val_loss: 0.2830\n",
      "Epoch 142/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2754 - val_loss: 0.2748\n",
      "Epoch 143/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2730 - val_loss: 0.2714\n",
      "Epoch 144/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2731 - val_loss: 0.2743\n",
      "Epoch 145/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2695 - val_loss: 0.2670\n",
      "Epoch 146/150\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.2680 - val_loss: 0.2678\n",
      "Epoch 147/150\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.2653 - val_loss: 0.2652\n",
      "Epoch 148/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2632 - val_loss: 0.2640\n",
      "Epoch 149/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2623 - val_loss: 0.2606\n",
      "Epoch 150/150\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.2593 - val_loss: 0.2616\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), loss='categorical_crossentropy')\n",
    "history = model.fit(X_train, Y_train_encoded, epochs=150, validation_data=(X_test, Y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1aba03b24c0>]"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 13})\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(range(len(history.history['loss'])), history.history['loss'])\n",
    "ax.plot(range(len(history.history['val_loss'])), history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n",
      "train accuracy: 0.9666666666666667\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.argmax(model.predict(X_train), axis=1)\n",
    "print('train accuracy:', accuracy_score(Y_train, y_pred))\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "print('test accuracy:', accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, use logistic regression provided by sklearn to predict result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "clf.fit(X_train, Y_train.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9833333333333333\n",
      "test accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_train)\n",
    "print('train accuracy:', accuracy_score(Y_train, y_pred))\n",
    "y_pred = clf.predict(X_test)\n",
    "print('test accuracy:', accuracy_score(Y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f477485d285a5f10682b4eba39a7346cdf553c3edeb76419d766de0b2758d1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
